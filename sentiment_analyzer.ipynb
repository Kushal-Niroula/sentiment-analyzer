{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF9vL3bosCp3",
        "colab_type": "text"
      },
      "source": [
        "#imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iQgmil111VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN3WdTgisO2C",
        "colab_type": "text"
      },
      "source": [
        "#Defining text and label field\n",
        " We are using spacy tokenizer. It takes a string and tokenizes every words and punctuations. We can further use this tokens to create our vocabulary for the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXN3dlVDyj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP3kr-7mtCIQ",
        "colab_type": "text"
      },
      "source": [
        "importing imdb movie review dataset from torchtext library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ald6gD-GIz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0e405ed-ceba-4ef1-fd39-7e9608241b4b"
      },
      "source": [
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CoqKY2DtVet",
        "colab_type": "text"
      },
      "source": [
        "splitting data into training and validation set. since our dataset is large we can afford to split 50% for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2sopS2RXM2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "train_data , valid_data = train_data.split(random_state = random.seed(123))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13_ryO9tgpc",
        "colab_type": "text"
      },
      "source": [
        "example of our training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWCI0umZImda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d759eb73-c970-42e5-efe9-a9f44e7d288a"
      },
      "source": [
        "vars(train_data[-1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'neg',\n",
              " 'text': ['Man',\n",
              "  ',',\n",
              "  'I',\n",
              "  'really',\n",
              "  'find',\n",
              "  'it',\n",
              "  'hard',\n",
              "  'to',\n",
              "  'believe',\n",
              "  'that',\n",
              "  'the',\n",
              "  'wonderful',\n",
              "  'Alan',\n",
              "  'Ball',\n",
              "  'had',\n",
              "  'anything',\n",
              "  'to',\n",
              "  'do',\n",
              "  'with',\n",
              "  'this',\n",
              "  'mess',\n",
              "  '.',\n",
              "  'Having',\n",
              "  'seen',\n",
              "  'the',\n",
              "  'first',\n",
              "  'two',\n",
              "  'episodes',\n",
              "  'thus',\n",
              "  'far',\n",
              "  ',',\n",
              "  'I',\n",
              "  'think',\n",
              "  'I',\n",
              "  'can',\n",
              "  'safely',\n",
              "  'say',\n",
              "  'this',\n",
              "  'show',\n",
              "  'is',\n",
              "  \"n't\",\n",
              "  'going',\n",
              "  'to',\n",
              "  'be',\n",
              "  'on',\n",
              "  'my',\n",
              "  'must',\n",
              "  'see',\n",
              "  'list',\n",
              "  '.',\n",
              "  'It',\n",
              "  \"'s\",\n",
              "  'just',\n",
              "  'got',\n",
              "  'so',\n",
              "  'many',\n",
              "  'things',\n",
              "  'working',\n",
              "  'against',\n",
              "  'it.<br',\n",
              "  '/><br',\n",
              "  '/>None',\n",
              "  'of',\n",
              "  'the',\n",
              "  'actors',\n",
              "  'cast',\n",
              "  'are',\n",
              "  'particularly',\n",
              "  'good',\n",
              "  '.',\n",
              "  'Anna',\n",
              "  'Paquin',\n",
              "  'as',\n",
              "  'the',\n",
              "  'lead',\n",
              "  'character',\n",
              "  'Sookie',\n",
              "  ',',\n",
              "  'is',\n",
              "  'just',\n",
              "  'awful',\n",
              "  '.',\n",
              "  'I',\n",
              "  'remember',\n",
              "  'her',\n",
              "  'being',\n",
              "  'better',\n",
              "  'in',\n",
              "  'a',\n",
              "  'lot',\n",
              "  'of',\n",
              "  'other',\n",
              "  'things',\n",
              "  'I',\n",
              "  \"'ve\",\n",
              "  'seen',\n",
              "  'her',\n",
              "  'in',\n",
              "  'so',\n",
              "  'maybe',\n",
              "  'it',\n",
              "  \"'s\",\n",
              "  'just',\n",
              "  'the',\n",
              "  'writing',\n",
              "  '.',\n",
              "  'She',\n",
              "  \"'s\",\n",
              "  'not',\n",
              "  'really',\n",
              "  'much',\n",
              "  'fun',\n",
              "  'to',\n",
              "  'look',\n",
              "  'at',\n",
              "  'either',\n",
              "  ',',\n",
              "  'there',\n",
              "  'are',\n",
              "  'moments',\n",
              "  'where',\n",
              "  'to',\n",
              "  'be',\n",
              "  'honest',\n",
              "  'she',\n",
              "  'looks',\n",
              "  'downright',\n",
              "  'ugly',\n",
              "  '.',\n",
              "  'The',\n",
              "  'actor',\n",
              "  'who',\n",
              "  'plays',\n",
              "  'Bill',\n",
              "  'is',\n",
              "  'marginally',\n",
              "  'better',\n",
              "  ',',\n",
              "  'if',\n",
              "  'only',\n",
              "  'because',\n",
              "  'his',\n",
              "  'character',\n",
              "  'is',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'be',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'wooden',\n",
              "  'and',\n",
              "  'aloof',\n",
              "  '.',\n",
              "  'The',\n",
              "  'other',\n",
              "  'actors',\n",
              "  'do',\n",
              "  'their',\n",
              "  'best',\n",
              "  'but',\n",
              "  'with',\n",
              "  'the',\n",
              "  'cliché',\n",
              "  'characters',\n",
              "  'with',\n",
              "  'difficult',\n",
              "  'to',\n",
              "  'perform',\n",
              "  'accents',\n",
              "  'they',\n",
              "  'are',\n",
              "  'given',\n",
              "  'it',\n",
              "  \"'s\",\n",
              "  'a',\n",
              "  'tough',\n",
              "  'job',\n",
              "  '.',\n",
              "  'Tara',\n",
              "  'is',\n",
              "  'an',\n",
              "  'absolute',\n",
              "  'misery',\n",
              "  'to',\n",
              "  'watch',\n",
              "  ',',\n",
              "  'Rutina',\n",
              "  'Wesley',\n",
              "  'absolutely',\n",
              "  'murders',\n",
              "  'the',\n",
              "  'accent',\n",
              "  '.',\n",
              "  'It',\n",
              "  \"'s\",\n",
              "  'like',\n",
              "  'nails',\n",
              "  'on',\n",
              "  'a',\n",
              "  'chalkboard',\n",
              "  'bad',\n",
              "  '.',\n",
              "  'Almost',\n",
              "  'as',\n",
              "  'awful',\n",
              "  'is',\n",
              "  'Nelsan',\n",
              "  'Ellis',\n",
              "  ',',\n",
              "  'it',\n",
              "  \"'s\",\n",
              "  'difficult',\n",
              "  'to',\n",
              "  'understand',\n",
              "  'what',\n",
              "  'he',\n",
              "  \"'s\",\n",
              "  'even',\n",
              "  'saying',\n",
              "  'sometimes',\n",
              "  '.',\n",
              "  'Both',\n",
              "  'his',\n",
              "  'character',\n",
              "  'as',\n",
              "  'well',\n",
              "  'as',\n",
              "  'Tara',\n",
              "  \"'s\",\n",
              "  'also',\n",
              "  'seem',\n",
              "  'a',\n",
              "  'bit',\n",
              "  'racist',\n",
              "  'to',\n",
              "  'me',\n",
              "  '.',\n",
              "  'I',\n",
              "  'do',\n",
              "  \"n't\",\n",
              "  'know',\n",
              "  ',',\n",
              "  'having',\n",
              "  'a',\n",
              "  'character',\n",
              "  'say',\n",
              "  \"'\",\n",
              "  'whycome',\n",
              "  \"'\",\n",
              "  'on',\n",
              "  'an',\n",
              "  'HBO',\n",
              "  'show',\n",
              "  'that',\n",
              "  'is',\n",
              "  \"n't\",\n",
              "  'The',\n",
              "  'Wire',\n",
              "  'just',\n",
              "  'seems',\n",
              "  'a',\n",
              "  'bit',\n",
              "  'odd',\n",
              "  '.',\n",
              "  'Rounding',\n",
              "  'out',\n",
              "  'the',\n",
              "  'cast',\n",
              "  'so',\n",
              "  'far',\n",
              "  'are',\n",
              "  'Sookie',\n",
              "  \"'s\",\n",
              "  'doddering',\n",
              "  'grandmother',\n",
              "  ',',\n",
              "  'her',\n",
              "  'sex',\n",
              "  'addict',\n",
              "  'brother',\n",
              "  ',',\n",
              "  'and',\n",
              "  'the',\n",
              "  'only',\n",
              "  'bit',\n",
              "  'of',\n",
              "  'genius',\n",
              "  'casting',\n",
              "  'I',\n",
              "  \"'ve\",\n",
              "  'seen',\n",
              "  'in',\n",
              "  'William',\n",
              "  'Sanderson',\n",
              "  'as',\n",
              "  'the',\n",
              "  'sheriff.<br',\n",
              "  '/><br',\n",
              "  '/>The',\n",
              "  'story',\n",
              "  'seems',\n",
              "  'to',\n",
              "  'be',\n",
              "  'meandering',\n",
              "  'towards',\n",
              "  'it',\n",
              "  \"'s\",\n",
              "  'destination',\n",
              "  'at',\n",
              "  'this',\n",
              "  'point',\n",
              "  ',',\n",
              "  'with',\n",
              "  'no',\n",
              "  'real',\n",
              "  'worry',\n",
              "  'about',\n",
              "  'keeping',\n",
              "  'the',\n",
              "  'viewer',\n",
              "  'interested',\n",
              "  '.',\n",
              "  'The',\n",
              "  'romance',\n",
              "  'stuff',\n",
              "  'is',\n",
              "  'very',\n",
              "  'Dark',\n",
              "  'Shadow',\n",
              "  '-',\n",
              "  'sy',\n",
              "  '.',\n",
              "  'Although',\n",
              "  'this',\n",
              "  'show',\n",
              "  'ups',\n",
              "  'the',\n",
              "  'camp',\n",
              "  'factor',\n",
              "  'from',\n",
              "  'something',\n",
              "  'like',\n",
              "  'those',\n",
              "  'old',\n",
              "  'Dark',\n",
              "  'Shadows',\n",
              "  'episodes',\n",
              "  'times',\n",
              "  'about',\n",
              "  'ten',\n",
              "  '.',\n",
              "  'At',\n",
              "  'times',\n",
              "  'it',\n",
              "  'seemed',\n",
              "  'so',\n",
              "  'campy',\n",
              "  'to',\n",
              "  'me',\n",
              "  ',',\n",
              "  'that',\n",
              "  'I',\n",
              "  'just',\n",
              "  'have',\n",
              "  'to',\n",
              "  'assume',\n",
              "  'it',\n",
              "  'was',\n",
              "  'intended',\n",
              "  'to',\n",
              "  'be',\n",
              "  '.',\n",
              "  'But',\n",
              "  'unlike',\n",
              "  'a',\n",
              "  'show',\n",
              "  'such',\n",
              "  'as',\n",
              "  'Buffy',\n",
              "  ',',\n",
              "  'that',\n",
              "  'pulled',\n",
              "  'camp',\n",
              "  'off',\n",
              "  'masterfully',\n",
              "  ',',\n",
              "  'this',\n",
              "  'show',\n",
              "  'does',\n",
              "  'not',\n",
              "  '.',\n",
              "  'Out',\n",
              "  'of',\n",
              "  'place',\n",
              "  'with',\n",
              "  'the',\n",
              "  'campiness',\n",
              "  'is',\n",
              "  'the',\n",
              "  'extreme',\n",
              "  'gore',\n",
              "  'and',\n",
              "  'graphic',\n",
              "  'sex',\n",
              "  'of',\n",
              "  'the',\n",
              "  'show',\n",
              "  '.',\n",
              "  'I',\n",
              "  \"'m\",\n",
              "  'not',\n",
              "  'averse',\n",
              "  'to',\n",
              "  'either',\n",
              "  'of',\n",
              "  'these',\n",
              "  'when',\n",
              "  'they',\n",
              "  'are',\n",
              "  'done',\n",
              "  'well',\n",
              "  ',',\n",
              "  'as',\n",
              "  'they',\n",
              "  'have',\n",
              "  'in',\n",
              "  'many',\n",
              "  'other',\n",
              "  'HBO',\n",
              "  'shows',\n",
              "  'but',\n",
              "  'here',\n",
              "  'at',\n",
              "  'least',\n",
              "  'they',\n",
              "  'prolonged',\n",
              "  'rough',\n",
              "  'sex',\n",
              "  'scenes',\n",
              "  'involving',\n",
              "  'Jason',\n",
              "  'Stackhouse',\n",
              "  'seem',\n",
              "  'a',\n",
              "  'bit',\n",
              "  'over',\n",
              "  'the',\n",
              "  'top',\n",
              "  'and',\n",
              "  'pointless.<br',\n",
              "  '/><br',\n",
              "  '/>About',\n",
              "  'the',\n",
              "  'only',\n",
              "  'nice',\n",
              "  'thing',\n",
              "  'I',\n",
              "  'can',\n",
              "  'really',\n",
              "  'think',\n",
              "  'to',\n",
              "  'say',\n",
              "  'about',\n",
              "  'this',\n",
              "  'mess',\n",
              "  'is',\n",
              "  'that',\n",
              "  'I',\n",
              "  'liked',\n",
              "  'the',\n",
              "  'opening',\n",
              "  'title',\n",
              "  'sequence',\n",
              "  '.',\n",
              "  'HBO',\n",
              "  'has',\n",
              "  'had',\n",
              "  'a',\n",
              "  'string',\n",
              "  'of',\n",
              "  'bad',\n",
              "  'luck',\n",
              "  'with',\n",
              "  'their',\n",
              "  'shows',\n",
              "  'lately',\n",
              "  ',',\n",
              "  'I',\n",
              "  'hope',\n",
              "  'they',\n",
              "  'cancel',\n",
              "  'this',\n",
              "  'after',\n",
              "  'the',\n",
              "  'first',\n",
              "  'season',\n",
              "  'and',\n",
              "  'try',\n",
              "  'to',\n",
              "  'get',\n",
              "  'something',\n",
              "  'better',\n",
              "  'on',\n",
              "  'the',\n",
              "  'air',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3FgzKmtpga",
        "colab_type": "text"
      },
      "source": [
        "creating vocabulary for our model. we set maximum vocabulary size to 25000. Hence our model tokenizes 25000 unique tokens in our model and anything else will be marked with  unknown token. We are using pretrained embedding vector 'glove.6B.100d' prepared by stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FruduEHp0Oy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce8fa91c-affa-46f1-e8d5-39528da76837"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 398552/400000 [00:23<00:00, 17992.02it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdkfVtOZudYs",
        "colab_type": "text"
      },
      "source": [
        "creating an iterator for training. We are using BucketIterator for the job. And assining gpu as our primary processor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8manZlnvIlt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbrMQBv90g6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smCnEBZju0k4",
        "colab_type": "text"
      },
      "source": [
        "defining the model. we are using  bidirectional LSTM(long short term memory) model ie. our model passes through  the through first to last token and back in a sentence . Thus in the fully connected layer we have defined dimension (hidden_dim * 2,output_dim) . hidden_dim*2 because  of the biderectional nature of our lstm model .We have given the padding idx for the model to determine on which tokens not to train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIXUFnJy2_MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "       \n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        \n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prM8m7ZFwM2-",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3SMPBVQ3VDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfcwKZqjwbVe",
        "colab_type": "text"
      },
      "source": [
        "our pretrained embedding. From 'glove.6B.100D vectors' . since we defined max vocabulary as 25000 we have 25002 tokens. The other two being unknown and padding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrHgVOI3f4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bb227d2-3a0a-4df2-f1cc-af21c31328fd"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB89L-0nww1U",
        "colab_type": "text"
      },
      "source": [
        "copying our pretrained embedding to our model's embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLAhthp63nqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "957ef8c4-b063-413c-f43a-0b8594ce3255"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2208,  0.5414,  0.6170,  ...,  0.3331,  1.0870, -1.4192],\n",
              "        [ 1.2872, -0.0612,  0.1770,  ..., -0.8446,  1.5656,  0.1120],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.5454,  0.3363, -1.1108,  ..., -0.4860,  0.9797, -0.3946],\n",
              "        [-0.2630,  0.1020,  1.2268,  ...,  0.3066, -0.8744,  0.9514],\n",
              "        [ 1.2182, -1.1912,  0.0040,  ..., -0.0603, -1.9024, -0.9452]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoKgpA6Jw33x",
        "colab_type": "text"
      },
      "source": [
        "changing the values of unknown tokens and padding tokens to zeroes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHmNGabN3xZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9183b59b-b115-4db7-a833-fd0131346560"
      },
      "source": [
        "\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.5454,  0.3363, -1.1108,  ..., -0.4860,  0.9797, -0.3946],\n",
            "        [-0.2630,  0.1020,  1.2268,  ...,  0.3066, -0.8744,  0.9514],\n",
            "        [ 1.2182, -1.1912,  0.0040,  ..., -0.0603, -1.9024, -0.9452]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyd-_vpOxI25",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpIFeioj301Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moJxtKn8xQ_Z",
        "colab_type": "text"
      },
      "source": [
        "defining loss function and loading the model and loss function to cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RGaCHDI34iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vIonpAxZNz",
        "colab_type": "text"
      },
      "source": [
        "Function to determine accuracy of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mYB_51137pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ9d8C7TxkJU",
        "colab_type": "text"
      },
      "source": [
        "Function for training. The weights of the parameters are updated after each batch of 64 training samples since we are using iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGhcAjki4GnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8ck2dkyErX",
        "colab_type": "text"
      },
      "source": [
        "evaluation function for our validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i38bi8w4LnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhF--2l24X71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDuPOTzgyJcY",
        "colab_type": "text"
      },
      "source": [
        "Training for 5 epochs and saving the model with best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMfO_E8r4c_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f43e6745-f9d5-49aa-b5d5-13c7c337d14e"
      },
      "source": [
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    \n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.530 | Train Acc: 73.75%\n",
            "\t Val. Loss: 0.419 |  Val. Acc: 81.05%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.459 | Train Acc: 78.33%\n",
            "\t Val. Loss: 0.474 |  Val. Acc: 78.93%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.360 | Train Acc: 84.74%\n",
            "\t Val. Loss: 0.315 |  Val. Acc: 87.40%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.340 | Train Acc: 85.87%\n",
            "\t Val. Loss: 0.309 |  Val. Acc: 87.47%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.263 | Train Acc: 89.50%\n",
            "\t Val. Loss: 0.285 |  Val. Acc: 88.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo_KVNO3yXKH",
        "colab_type": "text"
      },
      "source": [
        "loading the best performing model and using the model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knwPhxnf4kou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6e7dc90-3d67-40fe-e879-43e73918c676"
      },
      "source": [
        "\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.288 | Test Acc: 88.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQ7rIpnygTq",
        "colab_type": "text"
      },
      "source": [
        "function to predict the sentiment of our custom statement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F1K5EdP5d29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7NWMArozIXQ",
        "colab_type": "text"
      },
      "source": [
        "trying custom prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv4AYOet5nsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f611971-3cdd-49dd-9491-d4dd29a5998c"
      },
      "source": [
        "predict_sentiment(model, \"i loved this movie\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9853605628013611"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3YE8GAR5v0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c88e28da-bfb9-432e-fff6-a4feb76c4f4b"
      },
      "source": [
        "predict_sentiment(model, \"I want my money back that i paid for this movie\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.042114295065402985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVZ8tjun50aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}