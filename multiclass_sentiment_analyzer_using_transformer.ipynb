{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgE8qoTsjXOM",
        "colab_type": "text"
      },
      "source": [
        "Installing transformer using pip package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWxRCzqm89lE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "7818a529-0f1b-47d3-ce9d-8dbdaf3b6d61"
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yipfo_7yjd31",
        "colab_type": "text"
      },
      "source": [
        "Basic imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQUZNuRj7A4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random \n",
        "import numpy as np \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwZzGpFj6cm",
        "colab_type": "text"
      },
      "source": [
        "The transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained bert-base-uncased tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGJcjFxK8XkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od5ApImT8tyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKj9UU7L9M4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15978adf-a71b-463b-ca46-fc50ae952b39"
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De5XpTTtkPQR",
        "colab_type": "text"
      },
      "source": [
        "This is how bert-base-uncased tokenizer tokenizes a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9HR-xx9RjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "592df540-659b-4a7a-ca4d-508e26f067e4"
      },
      "source": [
        "tokens = tokenizer.tokenize(\"macHine LEArning\")\n",
        "print(tokens)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['machine', 'learning']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIcfHLoX9Xl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60092ccb-3e77-42b7-d120-70b67e099927"
      },
      "source": [
        "id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(id)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3698, 4083]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRxVGZqHkiuk",
        "colab_type": "text"
      },
      "source": [
        "The transformer was also trained with special tokens to mark the beginning and end of the sentence. As well as a standard padding and unknown token. We can also get these from the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYpjsXhN9jyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49e58b6b-b117-4725-8f3b-31fba05d088a"
      },
      "source": [
        "\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqfWzHRuknoX",
        "colab_type": "text"
      },
      "source": [
        "ids of init , eos , padding and unk tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgWMeUYSbyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7ce62fa-eaa0-4861-ac6f-f6518aa1f4e0"
      },
      "source": [
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YE75k1QkxAK",
        "colab_type": "text"
      },
      "source": [
        "the transformer model was trained on fix maximum length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVVpuQVvSlFU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6bfa727-9a71-4282-a263-5e3bae7995a8"
      },
      "source": [
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRJCamK0k8BJ",
        "colab_type": "text"
      },
      "source": [
        "preprocessing function for our text to tokenize and cut it as per the max length. \n",
        "Note: Max_length-2 because we have init token and eos token  at the start and end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKMQcvlStta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qf3YXlflL9P",
        "colab_type": "text"
      },
      "source": [
        "Defining text field and label field. Label field is defined as long since multiclass categorical variable doesnt take float as a datatype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj6JZLHjSxEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.long)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ysCOfdjlQ9t",
        "colab_type": "text"
      },
      "source": [
        "splitting the dataset into trainset , test set and validset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHHq_eI5S2QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL)\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(1234))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh0Xm9m0lYxc",
        "colab_type": "text"
      },
      "source": [
        "number of samples in training , testing and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xbe8TUAS8wZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "84419c08-bc1f-404d-ef18-4ea5fcdad3c0"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 3816\n",
            "Number of validation examples: 1636\n",
            "Number of testing examples: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-oKq_akeycQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy0y0A2wlePf",
        "colab_type": "text"
      },
      "source": [
        "preparing iterators as per the batch size . For this job we use bucketiterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ-GO5BMTMUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATo-3uQNmCnj",
        "colab_type": "text"
      },
      "source": [
        "loading our pretrained model. Note we load same model as our tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auVMrSAoe2aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FppZgKcxmRdG",
        "colab_type": "text"
      },
      "source": [
        "defining our model. we will be using Biderectional , 2 layered  GRU mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqL_tsGve60k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-oF2MeoToy",
        "colab_type": "text"
      },
      "source": [
        "declaring hyperparameters of our model and instantiating our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5px3kcJfATl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHGSZ6MFod84",
        "colab_type": "text"
      },
      "source": [
        "freezing the pretrained parameters since we are using transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcoa6KZgfK6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msMXkyoGojIZ",
        "colab_type": "text"
      },
      "source": [
        "defining our optimizer and our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGlnsxwIfRY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_BraDi4fYhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiKr9nqUonWd",
        "colab_type": "text"
      },
      "source": [
        "accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVOnqG4ffsL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Xt9C0potKt",
        "colab_type": "text"
      },
      "source": [
        "Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESNhnJTBfvMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muPRQusgoxBe",
        "colab_type": "text"
      },
      "source": [
        "Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtdaHiZPf236",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqIM2XqQo0-f",
        "colab_type": "text"
      },
      "source": [
        "Function to test elapsed time in each epochs during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yUQFkYff8Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N0RqiYYo5CU",
        "colab_type": "text"
      },
      "source": [
        "Training..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKcAIFsBgAvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3472a33c-3036-42d6-ddd5-059ba08e4052"
      },
      "source": [
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 17s\n",
            "\tTrain Loss: 1.055 | Train Acc: 58.90%\n",
            "\t Val. Loss: 0.535 |  Val. Acc: 81.24%\n",
            "Epoch: 02 | Epoch Time: 0m 17s\n",
            "\tTrain Loss: 0.432 | Train Acc: 84.87%\n",
            "\t Val. Loss: 0.415 |  Val. Acc: 85.91%\n",
            "Epoch: 03 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.317 | Train Acc: 89.08%\n",
            "\t Val. Loss: 0.417 |  Val. Acc: 86.84%\n",
            "Epoch: 04 | Epoch Time: 0m 17s\n",
            "\tTrain Loss: 0.267 | Train Acc: 90.48%\n",
            "\t Val. Loss: 0.444 |  Val. Acc: 85.87%\n",
            "Epoch: 05 | Epoch Time: 0m 17s\n",
            "\tTrain Loss: 0.222 | Train Acc: 92.50%\n",
            "\t Val. Loss: 0.383 |  Val. Acc: 86.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iodmLh-pBkX",
        "colab_type": "text"
      },
      "source": [
        "Loading the best performing model and using it in our test case ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egKyt8SSgIX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b36a5899-f6d9-4cd5-92e9-05ecd6b7615a"
      },
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.277 | Test Acc: 91.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zStNC7tipH28",
        "colab_type": "text"
      },
      "source": [
        "Hurrayy! we achieved close to 92% accuracy for our test case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOKCOYXipRyD",
        "colab_type": "text"
      },
      "source": [
        "function to predict sentiment of our custom statement using our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTFst4lyhIOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict_class(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = model(tensor).argmax(dim=1)\n",
        "    return prediction.item()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ4pBeQpeIU",
        "colab_type": "text"
      },
      "source": [
        "Testing our custom statement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cimZ_X2QiAwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cd2c919-3177-4560-97c7-e0572302a19b"
      },
      "source": [
        "pred_class = predict_class(model,tokenizer, \"Who is Kushal?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 0 = HUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paRBaAVXiGdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36e8ead7-4f77-410a-867b-d2e79d2b5356"
      },
      "source": [
        "pred_class = predict_class(model,tokenizer, \"How many hours are there in a day?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 3 = NUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jefYoCoiLOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e04dd4f-948a-49f6-db38-ef5800e15259"
      },
      "source": [
        "pred_class = predict_class(model,tokenizer, \"Where is India?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 4 = LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3imhkbLiRuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27a3fa31-1101-4f19-8ae9-99207f439fda"
      },
      "source": [
        "\n",
        "pred_class = predict_class(model,tokenizer, \"What does UNO stands for?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 5 = ABBR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVTmU-RMi0av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 66,
      "outputs": []
    }
  ]
}